# TODO: This Snakefile needs to be completely refactored.
# Python standard library
from os.path import join
from os import listdir
import os, sys, re, datetime, json

# 3rd party imports from pypi
from snakemake.workflow import workflow as wf_api
from snakemake.utils import R

# Local imports
from scripts.common import (
    allocated,
    provided, 
    references,
    str_bool
)

# Timestamp in YYYYMMDD format
today = str(datetime.datetime.today()).split()[0].replace('-', '')

# Global workflow variables
configfile: "config.json"
samples  = config['samples']
workpath = config['project']['workpath']
tmpdir = config['options']['tmp_dir']

# Analysis options
# Run differential binding pipeline
run_dba = True 
if config['options']['contrasts'] == 'None':
    run_dba = False

# Read in resource information,
# containing information about 
# threads, mem, walltimes, etc.
# TODO: Add handler for when the
# mode is set to local.
with open(join('config', 'cluster.json')) as fh:
    cluster = json.load(fh)


def outputfiles2(groupslist, inputnorm):
    """
    Produces correct output filenames based on group information.
    Names will be:
    Inputnorm.Q5DD.RPGC.metagene_heatmap.pdf
    {groupName}.Q5DD.RPGC.metagene_heatmap.pdf
    {groupName}.sorted.RPGC.metagene_heatmap.pdf
    Note: Inputnorm will only be included when there are input samples.
    """
    dtoolgroups, dtoolext = [], []
    extensions = ["sorted.RPGC", "Q5DD.RPGC"]
    if len(inputnorm) == 2:
            dtoolgroups.extend(["InputNorm"])
            dtoolext.extend([extensions[1]])
    for group in groupslist:
            dtoolgroups.extend([group] * 2)
            dtoolext.extend([extensions[1], extensions[0]])
    if len(inputnorm) == 2:
            dtoolgroups.extend(["InputNorm.prot"])
            dtoolext.extend([extensions[1]])
    for group in groupslist:
            dtoolgroups.extend([group + ".prot"] * 2)
            dtoolext.extend([extensions[1], extensions[0]])
    return dtoolgroups, dtoolext


def outputfiles3(groupslist, inputnorm):
    """
    Produces correct output filenames based on group information.
    Names will be:
    Inputnorm.Q5DD.RPGC.Enh_heatmap.pdf
    {groupName}.Q5DD.RPGC.Enh_heatmap.pdf
    {groupName}.sorted.RPGC.Enh_heatmap.pdf
    Note: Inputnorm will only be included when there are input samples.
    This version does not make "Prot" outputs.
    """
    dtoolgroups, dtoolext = [], []
    extensions = ["sorted.RPGC", "Q5DD.RPGC"]
    if len(inputnorm) == 2:
            dtoolgroups.extend(["InputNorm"])
            dtoolext.extend([extensions[1]])
    for group in groupslist:
            dtoolgroups.extend([group] * 2)
            dtoolext.extend([extensions[1], extensions[0]])
    return dtoolgroups, dtoolext


se=""
pe=""
if config['project']['nends'] == 2 :
    pe="yes"
elif config['project']['nends'] == 1 :
    se="yes"

if pe == "yes":
    extensions = [ "sorted.RPGC", "Q5DD.RPGC" ]
    extensions2 = list(map(lambda x:re.sub(".RPGC","",x),extensions))
    extensions3 = { extensions2[i] + "." : "bam" for i in range(len(extensions2)) }
    extensions4 = [ extensions2[i] + ".bam" for i in range(len(extensions2)) ]
else:
    extensions = [ "sorted.RPGC", "Q5DD.RPGC" ]
    extensions2 = list(map(lambda x:re.sub(".RPGC","",x),extensions))
    types = [ "bam", "tagAlign.gz" ]
    extensions3 = { extensions2[i] + "." : types[i] for i in range(len(extensions2)) }
    extensions4 = [ extensions2[i] + "." + types[i] for i in range(len(extensions2)) ]

d = {i: True for i in [1,2,3]}

# Defining relationships between samples 
chip2input = config['project']['peaks']['inputs']
uniq_inputs = list(sorted(set([v for v in chip2input.values() if v])))

sampleswinput = []
for input in chip2input:
	if chip2input[input] != 'NA' and chip2input[input] != '':
		sampleswinput.append(input)

if len(sampleswinput) == 0:
    inputnorm = [""]
else:
    inputnorm = ["",".inputnorm"]

groupdata = config['project']['groups']

groupdatawinput = {}
for group, chips in groupdata.items() :
    tmp = [ ]
    for chip in chips :
        if chip in samples:
            tmp.append(chip)
            input = chip2input[chip]
            if input != 'NA' and input != '':
                tmp.append(input)
    if len(tmp) != 0:
        groupdatawinput[group]=set(tmp)

groups = list(groupdatawinput.keys())
deepgroups, deepexts = outputfiles2(groups,inputnorm)
deepgroups3, deepexts3 = outputfiles3(groups,inputnorm)
# Setup to run with ChIP samples, 
# which could include IgG samples
H3K4me3samples = list(config['project']['peaks']['chips'])

# Creating directories
# TODO: Remove this step as it's not needed,
# snakemake will automatically initialize any
# directories defined in each rule's output.
trim_dir='trim'
kraken_dir='kraken'
bam_dir='bam'
bw_dir='bigwig'
deeptools_dir='deeptools'
extra_fingerprint_dir='deeptools/sorted_fingerprint'
qc_dir="QC"
cfTool_dir="cfChIPtool"
cfTool_subdir1="cfChIPtool/BED"
cfTool_subdir2="cfChIPtool/BED/H3K4me3"
cfTool_subdir3="cfChIPtool/Samples"
cfTool_subdir4="cfChIPtool/Samples/H3K4me3"


for d in [trim_dir,kraken_dir,bam_dir,bw_dir,deeptools_dir,extra_fingerprint_dir,qc_dir,cfTool_dir, cfTool_subdir1, cfTool_subdir2, cfTool_subdir3, cfTool_subdir4]:
	if not os.path.exists(join(workpath,d)):
		os.mkdir(join(workpath,d))

cfChIP="yes"

# Final output files of the pipeline
rule all:
    input: 
        # QC documents
        join(workpath,"Reports","multiqc_report.html"),
        expand(join(workpath,"FQscreen","{name}.R{rn}.trim_screen.png"),name=samples,rn=[1,2]),
        expand(join(workpath,"FQscreen2","{name}.R{rn}.trim_screen.png"),name=samples,rn=[1,2]),
        expand(join(workpath,kraken_dir,"{name}.trim.fastq.kraken_bacteria.krona.html"),name=samples),
        join(workpath,qc_dir,"QCTable.txt"),
        expand(join(workpath,qc_dir,"{name}.{ext}.insert_size_metrics.txt"),name=samples,ext=extensions2),
        # Align using BWA and dedup with Picard
        expand(join(workpath,bam_dir,"{name}.{ext}"),name=samples,ext=extensions4),
        # BWA --> BigWig
        expand(join(workpath,bw_dir,"{name}.{ext}.bw",),name=samples,ext=extensions),
        expand(join(workpath,bw_dir,"{name}.Q5DD.RPGC.inputnorm.bw",),name=sampleswinput),
        # PhantomPeakQualTools
        expand(join(workpath,bam_dir,"{name}.{ext}.ppqt"),name=samples,ext=extensions2),
        # deeptools
        expand(join(workpath,deeptools_dir,"spearman_heatmap.{ext}.pdf"),ext=extensions),
   	expand(join(workpath,deeptools_dir,"{group}.fingerprint.{ext}.pdf"),group=groups,ext=extensions2),
        expand(join(workpath,deeptools_dir,"{group}.metagene_heatmap.{ext}.pdf"), zip, group=deepgroups,ext=deepexts),
        # cfChIP-specific
        expand(join(workpath,cfTool_dir,"Output","H3K4me3","Signatures","{name}.Q5DD.csv"),name=H3K4me3samples),
        join(workpath,qc_dir,"H3K4me3_cfChIP_signature.txt"),
        # Differential Binding Pipeline Target
        provided([join(workpath, 'dba.done')], run_dba)


# trim, remove PolyX and remove BL reads
rule trim_pe:
    input:
        file1=join(workpath,"{name}.R1.fastq.gz"),
        file2=join(workpath,"{name}.R2.fastq.gz"),
    output:
        outfq1=temp(join(workpath,trim_dir,"{name}.R1.trim.fastq.gz")),
        outfq2=temp(join(workpath,trim_dir,"{name}.R2.trim.fastq.gz")),
    params:
        rname="trim",
        cutadaptver=config['tools']['CUTADAPTVER'],
        workpath=config['project']['workpath'],
        fastawithadaptersetd=join(workpath, config['shared_resources']['ADAPTERS_FASTA']),
        blacklistbwaindex=config['references']['BLACKLISTBWAINDEX'],
        picardver=config['tools']['PICARDVER'],
        bwaver=config['tools']['BWAVER'],
        samtoolsver=config['tools']['SAMTOOLSVER'],
        minlen=35,
        leadingquality=10,
        trailingquality=10,
        javaram="64g",
    threads: 16
    shell: """
    module load {params.cutadaptver};
    if [ ! -e /lscratch/$SLURM_JOBID ]; then mkdir /lscratch/$SLURM_JOBID ;fi
    cd /lscratch/$SLURM_JOBID
    sample=`echo {input.file1}|awk -F "/" '{{print $NF}}'|awk -F ".R1.fastq" '{{print $1}}'`
    cutadapt --pair-filter=any --nextseq-trim=2 --trim-n -n 5 -O 5 -q {params.leadingquality},{params.trailingquality} -m {params.minlen}:{params.minlen} -b file:{params.fastawithadaptersetd} -B file:{params.fastawithadaptersetd} -j {threads} -o ${{sample}}.R1.cutadapt.fastq -p ${{sample}}.R2.cutadapt.fastq {input.file1} {input.file2}
    module load {params.bwaver};
    module load {params.samtoolsver};
    module load {params.picardver};
    bwa mem -t {threads} {params.blacklistbwaindex} ${{sample}}.R1.cutadapt.fastq ${{sample}}.R2.cutadapt.fastq | samtools view -@{threads} -f4 -b -o ${{sample}}.bam
    java -Xmx{params.javaram} -jar $PICARDJARPATH/picard.jar SamToFastq \
    VALIDATION_STRINGENCY=SILENT \
    INPUT=${{sample}}.bam \
    FASTQ=${{sample}}.R1.cutadapt.noBL.fastq \
    SECOND_END_FASTQ=${{sample}}.R2.cutadapt.noBL.fastq \
    UNPAIRED_FASTQ=${{sample}}.unpaired.noBL.fastq
    pigz -p {threads} ${{sample}}.R1.cutadapt.noBL.fastq;
    pigz -p {threads} ${{sample}}.R2.cutadapt.noBL.fastq;
    mv /lscratch/$SLURM_JOBID/${{sample}}.R1.cutadapt.noBL.fastq.gz {output.outfq1};
    mv /lscratch/$SLURM_JOBID/${{sample}}.R2.cutadapt.noBL.fastq.gz {output.outfq2};
    """


rule kraken_pe:
    input:
        fq1 = join(workpath,trim_dir,"{name}.R1.trim.fastq.gz"),
        fq2 = join(workpath,trim_dir,"{name}.R2.trim.fastq.gz"),
    output:
        krakentaxa = join(workpath,kraken_dir,"{name}.trim.fastq.kraken_bacteria.taxa.txt"),
        kronahtml = join(workpath,kraken_dir,"{name}.trim.fastq.kraken_bacteria.krona.html"),
    params: 
        rname='kraken',
        prefix="{name}",
        outdir=join(workpath,kraken_dir),
        bacdb=config['shared_resources']['KRAKENBACDB'],
        krakenver=config['tools']['KRAKENVER'],
        kronatoolsver=config['tools']['KRONATOOLSVER'],
    threads: 32
    shell: """
    module load {params.krakenver};
    module load {params.kronatoolsver};
    if [ ! -d {params.outdir} ];then mkdir {params.outdir};fi
    cd /lscratch/$SLURM_JOBID;
    cp -rv {params.bacdb} /lscratch/$SLURM_JOBID/;
    kraken --db /lscratch/$SLURM_JOBID/`echo {params.bacdb}|awk -F "/" '{{print \$NF}}'` --fastq-input --gzip-compressed --threads {threads} --output /lscratch/$SLURM_JOBID/{params.prefix}.krakenout --preload --paired {input.fq1} {input.fq2}
    kraken-translate --mpa-format --db /lscratch/$SLURM_JOBID/`echo {params.bacdb}|awk -F "/" '{{print \$NF}}'` /lscratch/$SLURM_JOBID/{params.prefix}.krakenout |cut -f2|sort|uniq -c|sort -k1,1nr > /lscratch/$SLURM_JOBID/{params.prefix}.krakentaxa
    cut -f2,3 /lscratch/$SLURM_JOBID/{params.prefix}.krakenout | ktImportTaxonomy - -o /lscratch/$SLURM_JOBID/{params.prefix}.kronahtml
    mv /lscratch/$SLURM_JOBID/{params.prefix}.krakentaxa {output.krakentaxa}
    mv /lscratch/$SLURM_JOBID/{params.prefix}.kronahtml {output.kronahtml}
    """


rule BWA_PE:
    input:
        infq1 = join(workpath,trim_dir,"{name}.R1.trim.fastq.gz"),
        infq2 = join(workpath,trim_dir,"{name}.R2.trim.fastq.gz"),
    params:
        d=join(workpath,bam_dir),
        rname='bwa',
        reference=config['references']['BWA'],
        bwaver=config['tools']['BWAVER'],
        samtoolsver=config['tools']['SAMTOOLSVER'],
        script=join(workpath,"workflow","scripts","bam_filter_by_mapq.py"),
        pythonver="python/3.5"
    output:
        outbam1=join(workpath,bam_dir,"{name}.sorted.bam"), 
        outbam2=temp(join(workpath,bam_dir,"{name}.Q5.bam")),
        flagstat1=join(workpath,bam_dir,"{name}.sorted.bam.flagstat"),
        idxstat1=join(workpath,bam_dir,"{name}.sorted.bam.idxstat"),
        flagstat2=join(workpath,bam_dir,"{name}.Q5.bam.flagstat"),
        idxstat2=join(workpath,bam_dir,"{name}.Q5.bam.idxstat"),
    threads: 32
    shell: """
    module load {params.bwaver};
    module load {params.samtoolsver};
    module load {params.pythonver};
    cd /lscratch/$SLURM_JOBID;  #### edited
    bwa mem -t {threads} {params.reference} {input.infq1} {input.infq2} | \
    samtools sort -@{threads} -o {output.outbam1}
    samtools index {output.outbam1}
    samtools flagstat {output.outbam1} > {output.flagstat1}
    samtools idxstats {output.outbam1} > {output.idxstat1}
    #samtools view -b -q 6 {output.outbam1} -o {output.outbam2}
    python {params.script} -i {output.outbam1} -o {output.outbam2} -q 6
    samtools index {output.outbam2}
    samtools flagstat {output.outbam2} > {output.flagstat2}
    samtools idxstats {output.outbam2} > {output.idxstat2}
    """  

if cfChIP == "yes":
  rule picard_dedup:
    input: 
        bam2=join(workpath,bam_dir,"{name}.Q5.bam")
    output:
        out5=join(workpath,bam_dir,"{name}.Q5DD.bam"),
        out5f=join(workpath,bam_dir,"{name}.Q5DD.bam.flagstat"),
        out5i=join(workpath,bam_dir,"{name}.Q5DD.bam.idxstat"),
        out6=join(workpath,bam_dir,"{name}.bwa.Q5.duplic"),
        out7=temp(join(workpath,bam_dir,"{name}.Q5DD.tagAlign"))
    params:
        rname='dedup',
        picardver=config['tools']['PICARDVER'],
        samtoolsver=config['tools']['SAMTOOLSVER'],
        rver=config['tools']['RVER'],
        javaram='16g',
        tmpBam="{name}.Q5DD.withXY.bam",
        rscript=join(config['references']['cfChIP_TOOLS_SRC'], "bam2fragment.R")
    shell: """
    module load {params.samtoolsver};
    module load {params.picardver};
    module load {params.rver}; 
    if [ ! -e /lscratch/$SLURM_JOBID ]; then mkdir /lscratch/$SLURM_JOBID ;fi
    cd /lscratch/$SLURM_JOBID
    java -Xmx{params.javaram} \
      -jar $PICARDJARPATH/picard.jar MarkDuplicates \
      INPUT={input.bam2} \
      OUTPUT={params.tmpBam} \
      TMP_DIR=/lscratch/$SLURM_JOBID \
      VALIDATION_STRINGENCY=SILENT \
      REMOVE_DUPLICATES=true \
      METRICS_FILE={output.out6}
    samtools index {params.tmpBam}
    samtools view -b {params.tmpBam} chr{{1..22}} > {output.out5}
    samtools index {output.out5}
    samtools flagstat {output.out5} > {output.out5f}
    samtools idxstats {output.out5} > {output.out5i}
    Rscript {params.rscript} {params.tmpBam} {output.out7}
    """

rule insert_size:
    input:
        bam = lambda w : join(workpath,bam_dir,w.name + "." + w.ext + "." + extensions3[w.ext + "."])
    output:
        txt= join(workpath,qc_dir,"{name}.{ext}.insert_size_metrics.txt"),
        pdf= temp(join(workpath,qc_dir,"{name}.{ext}.insert_size_histogram.pdf")),
    params:
        rname="insert_size",
        picardver=config['tools']['PICARDVER'],
        javaram='16g',
    shell: """
    module load {params.picardver};
    java -Xmx{params.javaram} -jar $PICARDJARPATH/picard.jar CollectInsertSizeMetrics \
    INPUT={input.bam} OUTPUT={output.txt} H={output.pdf}
    """


rule ppqt:
    input:
        bam = lambda w : join(workpath,bam_dir,w.name + "." + w.ext + "." + extensions3[w.ext + "."])
    output:
        ppqt= join(workpath,bam_dir,"{name}.{ext}.ppqt"),
        pdf= join(workpath,bam_dir,"{name}.{ext}.pdf"),
    params:
        rname="ppqt",
        samtoolsver=config['tools']['SAMTOOLSVER'],
        ppqt_script=join(workpath,"workflow","scripts","ppqt","run_spp.R"),
        rver=config['tools']['RVER'],
    run:
        commoncmd="module load {params.samtoolsver};module load {params.rver};"
        if se=="yes":
            cmd="Rscript {params.ppqt_script} \
                -c={input.bam} -savp={output.pdf} -out={output.ppqt} -tmpdir=/lscratch/$SLURM_JOBID -rf;"
        elif pe=="yes":
            cmd="samtools view -b -f 66 -o /lscratch/$SLURM_JOBID/bam1.f66.bam {input.bam}; \
                samtools index /lscratch/$SLURM_JOBID/bam1.f66.bam; \
                Rscript {params.ppqt_script} \
                -c=/lscratch/$SLURM_JOBID/bam1.f66.bam -savp={output.pdf} -out={output.ppqt} \
                -tmpdir=/lscratch/$SLURM_JOBID -rf;"
        shell(commoncmd+cmd)


rule ppqt_process:
    input:
        lambda w: [ join(workpath,bam_dir,sample + "." + w.ext + ".ppqt") for sample in samples ],
    output:
        out=join(workpath,bam_dir,'{ext}.ppqt.txt'),
    params:
        rname="ppqt_process",
    run:
        o=open(output.out,'w')
        for inppqt in input:
            sample_name = inppqt.split("." + wildcards.ext)[0].split("/")[-1]
            if sample_name in uniq_inputs:
                o.write(sample_name + "\t" + "200" + "\n")
            else:
                file = list(map(lambda z:z.strip().split(),open(inppqt,'r').readlines()))
                ppqt_values = file[0][2].split(",")
                extenders = []
                for ppqt_value in ppqt_values:
                    if int(ppqt_value) > 150:
                        extenders.append(ppqt_value)
                if len(extenders) > 0:
                    o.write(sample_name + "\t" + extenders[0] + "\n")
                else:
                    print(sample_name)
                    print("All estimated fragments lengths were less than 150bp which will may cause the pipeline to fail.")
                    print("Potential causes include: wrong ref genome selected or low starting DNA.")
                    print("Assuming default estimated fragment length of 200bp.\n")
                    o.write(sample_name + "\t" + "200" + "\n")
        o.close()


rule bam2bw:
    input:
        bam=join(workpath,bam_dir,"{name}.{ext}.bam"),
        ppqt=join(workpath,bam_dir,"{ext}.ppqt.txt"),
    output:
        outbw=join(workpath,bw_dir,"{name}.{ext}.RPGC.bw"),
    params:
        rname="bam2bw",
        reflen=config['references']['REFLEN'],
        deeptoolsver=config['tools']['DEEPTOOLSVER'],
        effectivegenomesize=config['references']['EFFECTIVEGENOMESIZE'],
    run:
        lines=list(map(lambda x:x.strip().split("\t"),open(params.reflen).readlines()))
        genomelen=0
        chrs=[]
        includedchrs=[]
        excludedchrs=[]
        for chrom,l in lines:
            chrs.append(chrom)
            if not "_" in chrom and chrom!="chrX" and chrom!="chrM" and chrom!="chrY":
                includedchrs.append(chrom)
                genomelen+=int(l)
        excludedchrs=list(set(chrs)-set(includedchrs))
        commoncmd="module load {params.deeptoolsver};"
        cmd="bamCoverage --bam "+input.bam+" -o "+output.outbw+" --binSize 25 --smoothLength 75 --ignoreForNormalization "+" ".join(excludedchrs)+" --numberOfProcessors 32 --normalizeUsing RPGC --effectiveGenomeSize "+params.effectivegenomesize
        if pe=="yes":
            cmd+=" --centerReads"
        else:
            file = list(map(lambda z:z.strip().split(),open(input.ppqt,'r').readlines()))
            extender = [ ppqt[1] for ppqt in file if ppqt[0] == wildcards.name ] 
            cmd+=" -e "+extender[0]
        shell(commoncmd+cmd)


rule deeptools_prep:
    input:
        bw=expand(join(workpath,bw_dir,"{name}.{ext}.bw"),name=samples,ext=extensions),
        bam=expand(join(workpath,bam_dir,"{name}.{ext}.bam"),name=samples,ext=extensions2),
        bw2=expand(join(workpath,bw_dir,"{name}.Q5DD.RPGC.inputnorm.bw"),name=sampleswinput)
    output:
        temp(expand(join(workpath,bw_dir,"{ext}.deeptools_prep"),ext=extensions)),
        temp(expand(join(workpath,bam_dir,"{group}.{ext}.deeptools_prep"),group=groups,ext=extensions2)),
        temp(expand(join(workpath,bw_dir,"{group2}.{ext2}.deeptools_prep"), zip, group2=deepgroups,ext2=deepexts)),
    params:
        rname="deeptools_prep",
        batch="--mem=10g --time=1:00:00",
    threads: 1
    run:
        for x in extensions2:
            bws=list(filter(lambda z:z.endswith(x+".RPGC.bw"),input.bw))
            bams=list(filter(lambda z:z.endswith(x+".bam"),input.bam))
            labels=list(map(lambda z:re.sub("."+x+".RPGC.bw","",z),
                list(map(lambda z:os.path.basename(z),bws))))
            bws2=list(filter(lambda z:z.endswith(x+".RPGC.inputnorm.bw"),input.bw2))
            labels2=list(map(lambda z:re.sub("."+x+".RPGC.inputnorm.bw","",z),
                list(map(lambda z:os.path.basename(z),bws2))))
            o=open(join(workpath,bw_dir,x+".RPGC.deeptools_prep"),'w')
            o.write("%s\n"%(x+".RPGC"))
            o.write("%s\n"%(" ".join(bws)))
            o.write("%s\n"%(" ".join(labels)))
            o.close()
            if len(bws2) > 0:
                for i in ["","prot."]:
                    o4=open(join(workpath,bw_dir,"InputNorm."+i+x+".RPGC.deeptools_prep"),'w')
                    o4.write("%s\n"%(x+".RPGC.inputnorm"))
                    o4.write("%s\n"%(" ".join(bws2)))
                    o4.write("%s\n"%(" ".join(labels2)))
                    o4.close()
            for group in groups:
                iter = [ i for i in range(len(labels)) if labels[i] in groupdatawinput[group] ]
                bws3 = [ bws[i] for i in iter ]
                labels3 = [ labels[i] for i in iter ]
                bams3 = [ bams[i] for i in iter ]
                for i in ["","prot."]:
                    o2=open(join(workpath,bam_dir,group+"."+i+x+".deeptools_prep"),'w')
                    o2.write("%s\n"%(x))
                    o2.write("%s\n"%(" ".join(bams3)))
                    o2.write("%s\n"%(" ".join(labels3)))
                    o2.close()
                for i in ["","prot."]:
                    o3=open(join(workpath,bw_dir,group+"."+i+x+".RPGC.deeptools_prep"),'w')
                    o3.write("%s\n"%(x+".RPGC"))
                    o3.write("%s\n"%(" ".join(bws3)))
                    o3.write("%s\n"%(" ".join(labels3)))
                    o3.close()


rule deeptools_QC:
    input:
        join(workpath,bw_dir,"{ext}.deeptools_prep"),
    output:
        heatmap=join(workpath,deeptools_dir,"spearman_heatmap.{ext}.pdf"),
        pca=join(workpath,deeptools_dir,"pca.{ext}.pdf"),
	    npz=temp(join(workpath,deeptools_dir,"{ext}.npz")),
    params:
        rname="deeptools_QC",
        deeptoolsver=config['tools']['DEEPTOOLSVER'],
        png=lambda w: join(workpath,deeptools_dir,"spearman_heatmap." + w.ext + "_mqc.png")
    run:
        import re
        commoncmd="module load {params.deeptoolsver}; module load python/2.7;"
        listfile=list(map(lambda z:z.strip().split(),open(input[0],'r').readlines()))
        ext=listfile[0][0]
        bws=listfile[1]
        labels=listfile[2]
        cmd1="multiBigwigSummary bins -b "+" ".join(bws)+" -l "+" ".join(labels)+" -out "+output.npz
        cmd2="plotCorrelation -in "+output.npz+" -o "+output.heatmap+" -c 'spearman' -p 'heatmap' --skipZeros --removeOutliers"
        cmd4="plotPCA -in "+output.npz+" -o "+output.pca
        cmd5="plotCorrelation -in "+output.npz+" -o "+ params.png +" -c 'spearman' -p 'heatmap' --skipZeros --removeOutliers"
        shell(commoncmd+cmd1)
        shell(commoncmd+cmd2)
        shell(commoncmd+cmd4)
        if "Q5DD" in wildcards.ext:
            shell(commoncmd+cmd5)


rule deeptools_fingerprint:
    input:
        join(workpath,bam_dir,"{group}.sorted.deeptools_prep"),
    output:
        image=join(workpath,deeptools_dir,"{group}.fingerprint.sorted.pdf"),
        metrics=join(workpath,extra_fingerprint_dir,"{group}.fingerprint.metrics.sorted.tsv"),
    params:
        rname="deeptools_fingerprint",
        deeptoolsver=config['tools']['DEEPTOOLSVER'],
	nthreads="8"
    run:
        import re
        commoncmd= "module load python/2.7; module load deeptools/3.5.1; " ##edited
        listfile=list(map(lambda z:z.strip().split(),open(input[0],'r').readlines()))
        ext=listfile[0][0]
        bams=listfile[1]
        labels=listfile[2]
        cmd="plotFingerprint -b "+" ".join(bams)+" --labels "+" ".join(labels)+" -p "+params.nthreads+" --skipZeros --outQualityMetrics "+output.metrics+" --plotFile "+output.image 
        if se == "yes":
            cmd+=" -e 200"
        shell(commoncmd+cmd)


rule deeptools_fingerprint_Q5DD:
    input:
        join(workpath,bam_dir,"{group}.Q5DD.deeptools_prep")
    output:
        image=join(workpath,deeptools_dir,"{group}.fingerprint.Q5DD.pdf"),
        raw=temp(join(workpath,deeptools_dir,"{group}.fingerprint.raw.Q5DD.tab")),
        metrics=join(workpath,deeptools_dir,"{group}.fingerprint.metrics.Q5DD.tsv"),
    params:
        rname="deeptools_fingerprint_Q5DD",
        deeptoolsver=config['tools']['DEEPTOOLSVER'],
	nthreads="8"
    run:
        import re
        commoncmd="module load python/2.7; module load deeptools/3.5.1; " ##edited
        listfile=list(map(lambda z:z.strip().split(),open(input[0],'r').readlines()))
        ext=listfile[0][0]
        bams=listfile[1]
        labels=listfile[2]
        cmd="plotFingerprint -b "+" ".join(bams)+" --labels "+" ".join(labels)+" -p "+params.nthreads+" --skipZeros --outQualityMetrics "+output.metrics+" --plotFile "+output.image+" --outRawCounts "+output.raw
        if se == "yes":
            cmd+=" -e 200"
        shell(commoncmd+cmd)


rule deeptools_genes:
    input:
        join(workpath,bw_dir,"{group}.{ext}.deeptools_prep")
    output:
        metaheat=join(workpath,deeptools_dir,"{group}.metagene_heatmap.{ext}.pdf"),
        TSSheat=join(workpath,deeptools_dir,"{group}.TSS_heatmap.{ext}.pdf"),
        metaline=join(workpath,deeptools_dir,"{group}.metagene_profile.{ext}.pdf"),
        TSSline=join(workpath,deeptools_dir,"{group}.TSS_profile.{ext}.pdf"),
        metamat=temp(join(workpath,deeptools_dir,"{group}.metagene.{ext}.mat.gz")),
        TSSmat=temp(join(workpath,deeptools_dir,"{group}.TSS.{ext}.mat.gz")),
        bed=temp(join(workpath,deeptools_dir,"{group}.geneinfo.{ext}.bed")),
    params:
        rname="deeptools_genes",
        deeptoolsver=config['tools']['DEEPTOOLSVER'],
        prebed=config['references']['GENEINFO'],
        nthreads="16"
    run:
        import re
        commoncmd="module load {params.deeptoolsver}; module load python/2.7;"
        listfile=list(map(lambda z:z.strip().split(),open(input[0],'r').readlines()))
        ext=listfile[0][0]
        bws=listfile[1]
        labels=listfile[2]
        if "prot" in wildcards.group:
            cmd1="grep --line-buffered 'protein_coding' "+ params.prebed  +" | awk -v OFS='\t' -F'\t' '{{print $1, $2, $3, $5, \".\", $4}}' > "+output.bed
        else:
            cmd1="awk -v OFS='\t' -F'\t' '{{print $1, $2, $3, $5, \".\", $4}}' "+params.prebed+" > "+output.bed
        cmd2="computeMatrix scale-regions -S "+" ".join(bws)+" -R "+output.bed+" -p "+params.nthreads+" --upstream 1000 --regionBodyLength 2000 --downstream 1000 --skipZeros -o "+output.metamat+" --samplesLabel "+" ".join(labels)
        cmd3="computeMatrix reference-point -S "+" ".join(bws)+" -R "+output.bed+" -p "+params.nthreads+" --referencePoint TSS --upstream 3000 --downstream 3000 --skipZeros -o "+output.TSSmat+" --samplesLabel "+" ".join(labels)
        cmd4="plotHeatmap -m "+output.metamat+" -out "+output.metaheat+" --colorMap 'BuGn' --yAxisLabel 'average RPGC' --regionsLabel 'genes' --legendLocation 'none'"
        cmd5="plotHeatmap -m "+output.TSSmat+" -out "+output.TSSheat+" --colorMap 'BuPu' --yAxisLabel 'average RPGC' --regionsLabel 'genes' --legendLocation 'none'"
        cmd6="plotProfile -m "+output.metamat+" -out "+output.metaline+" --plotHeight 15 --plotWidth 15 --perGroup --yAxisLabel 'average RPGC' --plotType 'se' --legendLocation upper-right"
        cmd7="plotProfile -m "+output.TSSmat+" -out "+output.TSSline+" --plotHeight 15 --plotWidth 15 --perGroup --yAxisLabel 'average RPGC' --plotType 'se' --legendLocation upper-left"
        shell(commoncmd+cmd1)
        shell(commoncmd+cmd2)
        shell(commoncmd+cmd3)
        shell(commoncmd+cmd4)
        shell(commoncmd+cmd5)
        shell(commoncmd+cmd6)
        shell(commoncmd+cmd7)


rule inputnorm:
    input:
        chip = join(workpath,bw_dir,"{name}.Q5DD.RPGC.bw"),
        ctrl = lambda w : join(workpath,bw_dir,chip2input[w.name] + ".Q5DD.RPGC.bw")
    output:
        join(workpath,bw_dir,"{name}.Q5DD.RPGC.inputnorm.bw")
    params:
        rname="inputnorm",
        deeptoolsver=config['tools']['DEEPTOOLSVER'],
    shell: """
    module load {params.deeptoolsver};
    bigwigCompare --binSize 25 --outFileName {output} --outFileFormat 'bigwig' --bigwig1 {input.chip} --bigwig2 {input.ctrl} --operation 'subtract' --skipNonCoveredRegions -p 32;
	"""


rule preseq:
    params:
        rname = "preseq",
        preseqver=config['tools']['PRESEQVER'],
    input:
        bam = join(workpath,bam_dir,"{name}.sorted.bam"),
    output:
        ccurve = join(workpath,qc_dir,"{name}.ccurve"),
    shell: """
    module load {params.preseqver};
    preseq c_curve -B -o {output.ccurve} {input.bam}            
    """


rule NRF:
    input:
        bam=join(workpath,bam_dir,"{name}.sorted.bam"),
    params:
        rname='NRF',
        samtoolsver=config['tools']['SAMTOOLSVER'],
        rver=config['tools']['RVER'],
        preseqver=config['tools']['PRESEQVER'],
        nrfscript=join(workpath,"workflow","scripts","atac_nrf.py "),            
    output:
        preseq=join(workpath,qc_dir,"{name}.preseq.dat"),
        preseqlog=join(workpath,qc_dir,"{name}.preseq.log"),
        nrf=temp(join(workpath,qc_dir,"{name}.nrf")),
    threads: 16
    shell: """
    module load {params.preseqver};
    preseq lc_extrap -P -B -D -o {output.preseq} {input.bam} -seed 12345 -v -l 100000000000 2> {output.preseqlog}
    python {params.nrfscript} {output.preseqlog} > {output.nrf}
    """


rule rawfastqc:
    """
    Quality-control step to assess sequencing quality of the raw data prior removing
    adapter sequences. FastQC generates a set of basic statistics to identify problems
    that can arise during sequencing or library preparation.
    @Input:
        Raw FastQ files (scatter)
    @Output:
        FastQC report and zip file containing data quality information
    """
    input:
        R1=join(workpath,"{name}.R1.fastq.gz"),
        R2=join(workpath,"{name}.R2.fastq.gz"),
    output:
        join(workpath,"rawfastQC","{name}.R1_fastqc.zip"),
        join(workpath,"rawfastQC","{name}.R2_fastqc.zip"),
    params:
        rname='rawfastqc',
        outdir=join(workpath,"rawfastQC"),
    envmodules: 
        config['tools']['FASTQCVER']
    threads:
        int(allocated("threads", "rawfastqc", cluster))
    shell: """
    fastqc \\
        {input.R1} \\
        {input.R2} \\
        -t {threads} \\
        -o {params.outdir}
    """


rule fastqc:
    """
    Quality-control step to assess sequencing quality of the raw data after removing
    adapter sequences. This step is run after trim_pe rule. FastQC is run after adapter
    trimming to evalute if the adapter sequences were properly removed.
    @Input:
        Trimmed FastQ files (scatter)
    @Output:
        Trimmed FastQC reports and zip file containing data quality information
    """
    input:
        R1=join(workpath,trim_dir,"{name}.R1.trim.fastq.gz"),
        R2=join(workpath,trim_dir,"{name}.R2.trim.fastq.gz"),
    output:
        join(workpath,"fastQC","{name}.R1.trim_fastqc.zip"),
        join(workpath,"fastQC","{name}.R2.trim_fastqc.zip"),
    params:
        rname='fastqc',
        outdir=join(workpath,"fastQC"),
    envmodules: 
        config['tools']['FASTQCVER']
    threads:
        int(allocated("threads", "fastqc", cluster))
    shell: """
    fastqc \\
        {input.R1} \\
        {input.R2} \\
        -t {threads} \\
        -o {params.outdir}
    """


rule fastq_screen:
    """
    Quality-control step to screen for different sources of contamination.
    FastQ Screen compares your sequencing data to a set of different reference
    genomes to determine if there is contamination. It allows a user to see if
    the composition of your library matches what you expect.
    @Input:
        Trimmed FastQ files (scatter)
    @Output:
        FastQ Screen report and logfiles
    """
    input:
        file1=join(workpath,trim_dir,"{name}.R1.trim.fastq.gz"),
        file2=join(workpath,trim_dir,"{name}.R2.trim.fastq.gz"),
    output:
        out1=join(workpath,"FQscreen","{name}.R1.trim_screen.txt"),
        out2=join(workpath,"FQscreen","{name}.R1.trim_screen.png"),
        out3=join(workpath,"FQscreen","{name}.R2.trim_screen.txt"),
        out4=join(workpath,"FQscreen","{name}.R2.trim_screen.png"),
        out5=join(workpath,"FQscreen2","{name}.R1.trim_screen.txt"),
        out6=join(workpath,"FQscreen2","{name}.R1.trim_screen.png"),
        out7=join(workpath,"FQscreen2","{name}.R2.trim_screen.txt"),
        out8=join(workpath,"FQscreen2","{name}.R2.trim_screen.png")
    params:
        rname   = 'fqscreen',
        outdir  = join(workpath,"FQscreen"),
        outdir2 = join(workpath,"FQscreen2"),
        # Exposed Parameters: modify resources/fastq_screen{_2}.conf 
        # to change defaults locations to bowtie2 indices
        fastq_screen         = config['bin']['FASTQ_SCREEN'],
        fastq_screen_config1 = config['shared_resources']['FASTQ_SCREEN_CONFIG_P1'],
        fastq_screen_config2 = config['shared_resources']['FASTQ_SCREEN_CONFIG_P2'],
    envmodules:
        config['tools']['BOWTIE2VER'],
        config['tools']['PERLVER'],
    threads: 
        int(allocated("threads", "fastq_screen", cluster))
    shell: """
    # First pass of contamination screening
    {params.fastq_screen} \\
        --conf {params.fastq_screen_config1} \\
        --outdir {params.outdir} \\
        --threads {threads} \\
        --subset 1000000 \\
        --aligner bowtie2 \\
        --force \\
        {input.file1} \\
        {input.file2}
    # Second pass of contamination screening
    {params.fastq_screen} \\
        --conf {params.fastq_screen_config2} \\
        --outdir {params.outdir2} \\
        --threads {threads} \\
        --subset 1000000 \\
        --aligner bowtie2 \\
        --force \\
        {input.file1} \\
        {input.file2}
    """


rule QCstats:
    input:
        flagstat=join(workpath,bam_dir,"{name}.sorted.bam.flagstat"),
        infq=join(workpath,"{name}.R1.fastq.gz"),	
        ddflagstat=join(workpath,bam_dir,"{name}.Q5DD.bam.flagstat"),
        nrf=join(workpath,qc_dir,"{name}.nrf"),
        ppqt=join(workpath,bam_dir,"{name}.Q5DD.ppqt"),
        ppqt2=join(workpath,bam_dir,"Q5DD.ppqt.txt"),
    params:
        rname='QCstats',
        filterCollate=join(workpath,"workflow","scripts","filterMetrics"),   
    output:
        sampleQCfile=temp(join(workpath,qc_dir,"{name}.qcmetrics")),
    threads: 16
    shell: """
    # Number of reads
    #grep 'in total' {input.flagstat} | awk '{{print $1,$3}}' | {params.filterCollate} {wildcards.name} tnreads > {output.sampleQCfile}
    zcat {input.infq} | wc -l | {params.filterCollate} {wildcards.name} tnreads > {output.sampleQCfile}
    # Number of mapped reads
    grep 'mapped (' {input.flagstat} | awk '{{print $1,$3}}' | {params.filterCollate} {wildcards.name} mnreads >> {output.sampleQCfile}
    # Number of uniquely mapped reads
    grep 'mapped (' {input.ddflagstat} | awk '{{print $1,$3}}' | {params.filterCollate} {wildcards.name} unreads >> {output.sampleQCfile}
    # NRF, PCB1, PCB2
    cat {input.nrf} | {params.filterCollate} {wildcards.name} nrf >> {output.sampleQCfile}
    # NSC, RSC, Qtag
    awk '{{print $(NF-2),$(NF-1),$NF}}' {input.ppqt} | {params.filterCollate} {wildcards.name} ppqt >> {output.sampleQCfile}
    # Fragment Length
    fragLen=`grep {wildcards.name} {input.ppqt2} | cut -f 2`
    echo "{wildcards.name}\tFragmentLength\t$fragLen" >> {output.sampleQCfile}
    """


rule QCTable:
    input:
        expand(join(workpath,qc_dir,"{name}.qcmetrics"), name=samples),
    params:
        rname='QCTable',
        inputstring=" ".join(expand(join(workpath,qc_dir,"{name}.qcmetrics"), name=samples)),
        filterCollate=join(workpath,"workflow","scripts","createtable"),
    output:
        qctable=join(workpath,qc_dir,"QCTable.txt"),
    threads: 16
    shell: """
    cat {params.inputstring} | {params.filterCollate} > {output.qctable}
    """


rule multiqc:
    input: 
        expand(join(workpath,"FQscreen","{name}.R1.trim_screen.txt"),name=samples),
        expand(join(workpath,"FQscreen2","{name}.R1.trim_screen.txt"),name=samples),
        expand(join(workpath,qc_dir,"{name}.ccurve"), name=samples),
        expand(join(workpath,bam_dir,"{name}.Q5DD.bam.flagstat"), name=samples),
        expand(join(workpath,bam_dir,"{name}.Q5.bam.flagstat"), name=samples),
        join(workpath,qc_dir,"QCTable.txt"),
        expand(join(workpath,"rawfastQC","{name}.R1_fastqc.zip"),name=samples),
        expand(join(workpath,"fastQC","{name}.R1.trim_fastqc.zip"),name=samples),
        expand(join(workpath,deeptools_dir,"{group}.fingerprint.raw.Q5DD.tab"),group=groups),
        join(workpath,deeptools_dir,"spearman_heatmap.Q5DD.RPGC.pdf"),
    output:
        join(workpath,"Reports","multiqc_report.html")
    params:
        rname="multiqc",
        multiqc=config['tools']['MULTIQCVER'],
	qcconfig=join(workpath, config['shared_resources']['MULTIQC_CONFIG']),
	dir=join("..",extra_fingerprint_dir)
    shell: """
    module load {params.multiqc}
    cd Reports && multiqc -f -c {params.qcconfig} --interactive -e cutadapt --ignore {params.dir} -d ../
    """


# TODO: Remove hardcoded H3K4me3, add as a command line option
# from a list of available marks that are included with hg19
if cfChIP == "yes":
  rule cfChIPtool:
        input: 
            join(workpath,bam_dir,"{name}.Q5DD.tagAlign")
        output:
            out1=join(workpath,cfTool_subdir2,"{name}.Q5DD.tagAlign.gz"),
            out2=join(workpath,cfTool_dir,"Output","H3K4me3","Signatures","{name}.Q5DD.csv"),
        params:
            rname='cfChiP',
            rver="R/4.1.0",
            toolkit = config['references']['cfChIP_TOOLS_SRC'],
            tmpfile = lambda w: join(workpath,cfTool_subdir2, w.name + ".Q5DD.tagAlign"),
        container:
            config['images']['cfchip']
        shell: """
        cp {input} {params.tmpfile}
        gzip {params.tmpfile}
        Rscript {params.toolkit}/ProcessBEDFiles.R \\
            -a {params.toolkit}/SetupFiles/H3K4me3 \\
            -r {cfTool_dir} \\
            -p {cfTool_dir} \\
            -m H3K4me3 \\
            -S {output.out1}
        """

if cfChIP == "yes":
  rule cfChIPcompile:
        input:
            expand(join(workpath,cfTool_dir,"Output","H3K4me3","Signatures","{name}.Q5DD.csv"),name=H3K4me3samples)
        output:
            txt=join(workpath,qc_dir,"H3K4me3_cfChIP_signature.txt"),
            pdf=join(workpath,qc_dir,"H3K4me3_cfChIP_signature.pdf")
        params:
            rname="cfChIP2",
            script=join(workpath,"workflow","scripts","cfChIP_signatures.R"),
            infolder=join(workpath,cfTool_dir,"Output","H3K4me3","Signatures"),
        container:
            config['images']['cfchip']
        shell: """
        Rscript -e "source('{params.script}'); mergeSignatures( '{params.infolder}', '{output.txt}' )";
        Rscript -e "source('{params.script}'); plotSignatures( '{output.txt}', '{output.pdf}' )";
        """


rule deeptools_enhancer:
    input:
        join(workpath,bw_dir,"{group}.{ext}.deeptools_prep")
    output:
        Enhheat=join(workpath,deeptools_dir,"{group}.Enh_heatmap.{ext}.pdf"),
        Enhline=join(workpath,deeptools_dir,"{group}.Enh_profile.{ext}.pdf"),
        Enhmat=temp(join(workpath,deeptools_dir,"{group}.Enh.{ext}.mat.gz")),
    params:
        rname="deeptools_enhancer",
        deeptoolsver=config['tools']['DEEPTOOLSVER'],
        bed=join(workpath,"human_permissive_enhancers_phase_1_and_2.bed.gz"),
        nthreads="16"
    run:
        commoncmd="module load {params.deeptoolsver}; module load python/2.7;"
        listfile=list(map(lambda z:z.strip().split(),open(input[0],'r').readlines()))
        ext=listfile[0][0]
        bws=listfile[1]
        labels=listfile[2]
        cmd3="computeMatrix reference-point -S "+" ".join(bws)+" -R "+params.bed+" -p "+params.nthreads+" --referencePoint center --upstream 3000 --downstream 3000 --skipZeros -o "+output.Enhmat+" --samplesLabel "+" ".join(labels)
        cmd5="plotHeatmap -m "+output.Enhmat+" -out "+output.Enhheat+" --colorMap 'YlGn_r' --yAxisLabel 'average RPGC' --regionsLabel 'Enhancers' --legendLocation 'none'"
        cmd7="plotProfile -m "+output.Enhmat+" -out "+output.Enhline+" --plotHeight 15 --plotWidth 15 --perGroup --yAxisLabel 'average RPGC' --plotType 'se' --legendLocation upper-left --refPointLabel 'Enhancer' --regionsLabel 'Enhancers'"
        shell(commoncmd+cmd3)
        shell(commoncmd+cmd5)
        shell(commoncmd+cmd7)



# Import local rules
if run_dba:
    include: join("rules", "dba.smk")
